{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm as tq\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def train(model, train_dataloader, val_dataloader, criterion, epochs, learning_rate):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0,\n",
    "                                                num_training_steps = total_steps)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for idx, batch in enumerate(train_dataloader): # tqdm\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            correct_preds += (outputs.argmax(1) == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "            \n",
    "            # print ETA every batch\n",
    "            elapsed_time = time.time() - start_time\n",
    "            elapsed_time_per_step = elapsed_time / (idx + 1)\n",
    "            eta_dt = datetime.timedelta(seconds=int(elapsed_time_per_step * (len(train_dataloader) - idx - 1)))\n",
    "            eta = str(eta_dt)\n",
    "            spent_dt = datetime.timedelta(seconds=int(elapsed_time))   \n",
    "            spent = str(spent_dt)\n",
    "            print(f\"[TRAINING]  {idx+1}/{len(train_dataloader)} | loss : {loss.item():.4f} | time : {spent}-{str(spent_dt+eta_dt)} | eta : {eta}\", end='\\r')\n",
    "            # break\n",
    "        print()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc = correct_preds / total_preds\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        print(f\"Train loss {np.round(train_loss, 4)} accuracy {np.round(train_acc, 4)}\")\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Val   loss {np.round(val_loss, 4)} accuracy {np.round(val_acc, 4)}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss #\n",
    "            best_model = model #.state_dict()\n",
    "\n",
    "    return best_model, train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    #device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "\n",
    "            correct_preds += (outputs.argmax(1) == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "            \n",
    "            # print ETA every batch\n",
    "            elapsed_time = time.time() - start_time\n",
    "            elapsed_time_per_step = elapsed_time / (idx + 1)\n",
    "            eta_dt = datetime.timedelta(seconds=int(elapsed_time_per_step * (len(dataloader) - idx - 1)))\n",
    "            eta = str(eta_dt)\n",
    "            spent_dt = datetime.timedelta(seconds=int(elapsed_time))   \n",
    "            spent = str(spent_dt)\n",
    "            print(f\"[VALIDATION]  {idx+1}/{len(dataloader)} | time : {spent}-{str(spent_dt+eta_dt)} | eta : {eta}\", end='\\r')\n",
    "        print()\n",
    "    return loss / len(dataloader), correct_preds / total_preds\n",
    "\n",
    "\n",
    "def inference(model, dataloader):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "        #for idx, batch in enumerate(dataloader):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_preds.extend(outputs.argmax(1).tolist())\n",
    "            correct_preds += (outputs.argmax(1) == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def result(train_losses, train_accs, val_losses, val_accs):\n",
    "    epochs = range(len(train_losses))\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, train_accs, '.-')\n",
    "    plt.ylabel('train accuracy')\n",
    "    #plt.xticks(visible=False)\n",
    "\n",
    "    #ax2 = plt.subplot(2, 2, 3, sharex=ax1)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title('')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.plot(epochs, val_accs, '.-')\n",
    "    plt.ylabel('validation accuracy')\n",
    "    #plt.show()\n",
    "    #plt.savefig('result_acc.png')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, train_losses, '.-')\n",
    "    plt.ylabel('train loss')\n",
    "    #plt.xticks(visible=False)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.title('')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.plot(epochs, val_losses, '.-')\n",
    "    plt.ylabel('validation loss')\n",
    "    #plt.show()\n",
    "    plt.savefig('result.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_gpu",
   "language": "python",
   "name": "cuda_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
