{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4376bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import datasets\n",
    "datasets.logging.set_verbosity_error()\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm as tq\n",
    "\n",
    "NUM_TRAIN = 15000\n",
    "NUM_TEST  = 7500\n",
    "\n",
    "\n",
    "dataset_list = [\n",
    "    'imdb',    # sentiment classification on movie reviews\n",
    "    'snli',    # natural language inference (relationship between pairs of sentences)\n",
    "    'ag_news', # classification of news article topics\n",
    "]\n",
    "\n",
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, mode='train'):\n",
    "        self.tokenizer = tokenizer\n",
    "        assert mode in ['train', 'test']\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.dataset = load_dataset('imdb', split=f'train[:{NUM_TRAIN}]')\n",
    "        else:\n",
    "            self.dataset = load_dataset('imdb', split=f'test[:{NUM_TEST}]')\n",
    "            \n",
    "        self.X = self.__prepare_X()\n",
    "        self.Y = self.dataset['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.X[idx]['input_ids'], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.X[idx]['attention_mask'], dtype=torch.long)\n",
    "        label = torch.tensor(self.Y[idx], dtype=torch.long)\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "    def __prepare_X(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        def tokenize_function(batch):\n",
    "            return tokenizer(batch['text'], \n",
    "                             padding=\"max_length\", \n",
    "                             truncation=True, \n",
    "                             max_length=256,\n",
    "                             add_special_tokens=True,\n",
    "                             return_tensors='pt',\n",
    "                    )\n",
    "        return self.dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "    \n",
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, mode='train'):\n",
    "        self.tokenizer = tokenizer\n",
    "        assert mode in ['train', 'test']\n",
    "        self.mode = mode\n",
    "        def filter_func(example):\n",
    "            return example['label'] != -1\n",
    "        if self.mode == 'train':\n",
    "            self.dataset = load_dataset('snli', split=f'train')\n",
    "            self.dataset = self.dataset.filter(filter_func)[:NUM_TRAIN]\n",
    "        else:\n",
    "            self.dataset = load_dataset('snli', split=f'test')\n",
    "            self.dataset = self.dataset.filter(filter_func)[:NUM_TEST]\n",
    "            \n",
    "        if self.mode == 'train':\n",
    "            self.Y = torch.tensor(self.dataset['label'][:NUM_TRAIN])\n",
    "        elif self.mode == 'test':\n",
    "            self.Y = torch.tensor(self.dataset['label'][:NUM_TEST])\n",
    "        self.X = self.__prepare_X()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.X[idx]['input_ids'], dtype=torch.long).squeeze()\n",
    "        attention_mask = torch.tensor(self.X[idx]['attention_mask'], dtype=torch.long).squeeze()\n",
    "        label = torch.tensor(self.Y[idx], dtype=torch.long)\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "    def __prepare_X(self):\n",
    "        X = []\n",
    "        for premise, hypothesis in tq(zip(self.dataset['premise'], self.dataset['hypothesis']), total=len(self.dataset['premise'])):\n",
    "            combined_sentence = '[CLS]' +premise + '[SEP]' + hypothesis + '[SEP]'\n",
    "            encoded = self.tokenizer(\n",
    "                combined_sentence, \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                max_length=128,\n",
    "                add_special_tokens=False,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            X.append(encoded)\n",
    "        return X\n",
    "\n",
    "        \n",
    "class AGNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, mode='train'):\n",
    "        self.tokenizer = tokenizer\n",
    "        assert mode in ['train', 'test']\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.dataset = load_dataset('ag_news', split=f'train[:{NUM_TRAIN}]')\n",
    "        else:\n",
    "            self.dataset = load_dataset('ag_news', split=f'test[:{NUM_TEST}]')\n",
    "            \n",
    "        self.X = self.__prepare_X()\n",
    "        self.Y = self.dataset['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.X[idx]['input_ids'], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.X[idx]['attention_mask'], dtype=torch.long)\n",
    "        label = torch.tensor(self.Y[idx], dtype=torch.long)\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "    def __prepare_X(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        def tokenize_function(batch):\n",
    "            return tokenizer(batch['text'], \n",
    "                             padding=\"max_length\", \n",
    "                             truncation=True, \n",
    "                             max_length=128,\n",
    "                             add_special_tokens=True,\n",
    "                             return_tensors='pt'\n",
    "                    )\n",
    "        return self.dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    # load dataset\n",
    "    # imdb = load_dataset('imdb')\n",
    "    # imdb.keys() : ['train', 'test', 'unsupervised']\n",
    "    # imdb['train'][idx]['text'] : x\n",
    "    # imdb['train'][idx]['label'] : y\n",
    "\n",
    "    # snli = load_dataset('snli')\n",
    "    # snli.keys() : ['test', 'train', 'validation']\n",
    "    # snli['train']['premise']    : x1\n",
    "    # snli['train']['hypothesis'] : x2\n",
    "    # snli['train']['label]       : y\n",
    "\n",
    "    # labels : \n",
    "    #   - 0 : indicates the \"hypothesis\"(x2) entails the \"premise\"(x1) \n",
    "    #   - 1 : indicates the \"premise\"(x1) and \"hypothesis\"(x2) neither entail nor contracidct each other\n",
    "    #   - 2 : indicates the \"hypothesis\"(x2) contradicts the \"premise\"(x1)\n",
    "\n",
    "    # news = load_dataset('ag_news')\n",
    "    # news keys() : ['train', 'test']\n",
    "    # news['train'][idx]['text']  : x\n",
    "    # news['train'][idx]['label'] : y \n",
    "    # labels : World (0), Sports (1), Business (2), Sci/Tech (3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_gpu",
   "language": "python",
   "name": "cuda_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
