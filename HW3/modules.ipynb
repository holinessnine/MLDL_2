{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "##### Q2. Implement Baseline - Adding adapter module to all transformer layers #####################\n",
    " \n",
    "\"\"\"\n",
    "1: Implementing 'Top' Adapter\n",
    "NOTE:\n",
    "    In this section, your task is to design top adapter (similar to 'head') for the given datasets. \n",
    "    You have the flexibility to design the top adapter as you see fit, \n",
    "    but it is advised to avoid excessive complexity or resource-intensive configurations.\n",
    "    \n",
    "    Please refer to the [BERT] class provided below.\n",
    "\n",
    "CONSTRAINTS:\n",
    "    The number of output classes differ depending on the dataset. \n",
    "    Here are the specifications for each dataset:\n",
    "    - BERT encoder layer produces an output shape of [B, 512, 768], \n",
    "      where B represents the batch size, 512 represents the sequence length, and 768 represents the embedding dimension. \n",
    "    - For the IMDB dataset, the model should have [2 output class], and the loss function is CrossEntropyLoss.\n",
    "    - For the SNLI dataset, the model should have [3 output classes], and the loss function is CrossEntropyLoss.\n",
    "    - For the AGNews dataset, the model should have [4 output classes], and the loss function is CrossEntropyLoss.\n",
    "    \n",
    "    Keep in mind that an activation layer at the end of your implementation is NOT necessary. \n",
    "    This is due to the fact that the proposed loss functions already have built-in sigmoid or softmax functions.\n",
    "    \n",
    "HINT: \n",
    "    To implement the top adapter, \n",
    "    you might want to consider using a combination of [torch.nn.Linear] and activation layers (e.g., torch.nn.GELU())\n",
    "    or utilizing [torch.nn.Sequential].\n",
    "\"\"\"\n",
    "\n",
    "# ==== 1. Implement top adapter  =========================================\n",
    "class TopAdapter(torch.nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Q3. Modify - Changing the size of adapter (hidden unit):\n",
    "        self.adapter = torch.nn.Sequential(\n",
    "            ##### Fill here #####\n",
    "            torch.nn.Linear(768, num_classes),\n",
    "            torch.nn.GELU()\n",
    "            #####################\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.adapter ##### Fill here #####\n",
    "\n",
    "# ==== 1. Implement top adapter  =========================================\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2: Implementing 'Layer-Wise' Adapter\n",
    "NOTE:\n",
    "    In this section, your task is to design layer-wise adapter for BERT's encoder layers. \n",
    "    You have the flexibility to design the layer-wise adapter as you see fit, \n",
    "    but it is advised to avoid excessive complexity or resource-intensive configurations.\n",
    "    \n",
    "    Please refer to the [BERT] class provided below.\n",
    "\n",
    "CONSTRAINTS:\n",
    "    Please note that the dimensions for both input and output are 768,\n",
    "    and the BERT (base) model contains 12 layers.\n",
    "    \n",
    "HINT: \n",
    "    Consider employing [torch.nn.ModuleList] for the implementation of the layer-wise adapter.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# ==== 2. Implement layer-wise adapter  =========================================\n",
    "class LayerWiseAdapter(torch.nn.Module):\n",
    "    def __init__(self, num_adapters=12):\n",
    "        super().__init__()\n",
    "\n",
    "        # Q4. Modify - Adding adapter module to the top half layers only:\n",
    "        # HINT : original number of BERT layers is 12\n",
    "        self.num_adapters  = num_adapters\n",
    "        \n",
    "        # Q3. Modify - Changing the size of adapter (hidden unit):\n",
    "        self.adapter = torch.nn.ModuleList([\n",
    "            *[torch.nn.Sequential(\n",
    "                ##### Fill here #####\n",
    "                torch.nn.Linear(768, 768),\n",
    "                torch.nn.GELU(),\n",
    "                #####################\n",
    "            ) for _ in range(self.num_adapters)]\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, idx):\n",
    "        return self.adapter[idx](x) ##### Fill here #####\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_adapters\n",
    "\n",
    "# ==== 2. Implement layer-wise adapters  =========================================\n",
    "##### Q2 #########################################################################\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "class BERT(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 top_adapter:torch.nn.Module=None, \n",
    "                 layer_wise_adapters:torch.nn.ModuleList=None,\n",
    "                 num_classes=1,\n",
    "                 num_adapters=12,\n",
    "                 full_finetuning=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.full_finetuning = full_finetuning\n",
    "        self.num_adapters = num_adapters\n",
    "\n",
    "        \"\"\"load pretrained BERT \"\"\"\n",
    "        PLM = AutoModel.from_pretrained(\"bert-base-uncased\") \n",
    "        embeddings = PLM.embeddings\n",
    "        embeddings.eval()\n",
    "        layers = PLM.encoder.layer # torch.nn.ModuleList\n",
    "        layers.eval()\n",
    "\n",
    "        \"\"\"Our adaptable BERT model with interchangeable adapter layers\"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [layers[idx] for idx in range(len(layers))]\n",
    "        )\n",
    "        self.pooler = torch.nn.Linear(768, num_classes)\n",
    "\n",
    "        # Q1. Find and change - Full fine-tuning setting\n",
    "        # HINT: use 'requires_grad' method\n",
    "\n",
    "        # Freeze pretrained BERT embeddings\n",
    "        for param in self.embeddings.parameters():\n",
    "            param.requires_grad = self.full_finetuning\n",
    "\n",
    "        # Freeze pretrained BERT layers\n",
    "        for layer in self.layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = self.full_finetuning\n",
    "                \n",
    "        # interchangeable adapter layers\n",
    "        self.top_adapter = top_adapter # torch.nn.Module (single module)\n",
    "        self.layer_wise_adapters = layer_wise_adapters # torch.nn.ModuleList \n",
    "        if layer_wise_adapters is not None:\n",
    "            self.num_layer_wise_adapters = len(self.layer_wise_adapters) # int\n",
    "        else:\n",
    "            self.num_layer_wise_adapters = 0\n",
    "    \n",
    "    # Q4. Modify - Adding adapter module to the top half layers only:\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2).to(dtype=attention_mask.dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        # Convert the input tokens into embeddings\n",
    "        x = self.embeddings(input_ids) \n",
    "        # Process the embeddings through each layer in the model's layer stack\n",
    "        for idx in range(len(self.layers)):\n",
    "            # Apply the layer and the attention mask to the embeddings to generate hidden states\n",
    "            x = self.layers[idx](x,attention_mask=extended_attention_mask)[0]\n",
    "            \n",
    "            # If layer-wise adapters are defined, \n",
    "            # apply the corresponding adapter to the layer's output\n",
    "            # (2. Your layer_wise_adapters will be applied here)\n",
    "\n",
    "            if self.layer_wise_adapters is not None:\n",
    "                x = self.layer_wise_adapters(x, idx=self.num_adapters)\n",
    "        \n",
    "        last_hidden_state = x[:, 0, :]\n",
    "        if self.top_adapter is not None:\n",
    "            # If a top adapter is defined, apply it to the output of the final layer\n",
    "            # (1. Your top_adapter will be applied here)\n",
    "            out = self.top_adapter(last_hidden_state)\n",
    "        else:\n",
    "            # If no top adapter is defined, \n",
    "            # apply the pooling layer to the output of the final layer instead\n",
    "            out = self.pooler(last_hidden_state)\n",
    "        \n",
    "        return out\n",
    "# ====================================================================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_gpu",
   "language": "python",
   "name": "cuda_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
