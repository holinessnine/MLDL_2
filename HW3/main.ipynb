{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import warnings\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# private modules\n",
    "from dataset import IMDBDataset, SNLIDataset, AGNewsDataset\n",
    "from modules import BERT\n",
    "from modules import TopAdapter\n",
    "from modules import LayerWiseAdapter\n",
    "import functions\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "transformers.logging.set_verbosity_error()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\"\"\"\n",
    "HOW TO EXECUTE :\n",
    "    python main.py --dataset [dataset name] --top --layer --num_adapters 12 --full_finetuning\n",
    "\n",
    "    explanation : \n",
    "        --dataset [dataset name] : Specify the dataset (required)\n",
    "            - e.g. python main.py --dataset imdb --top --layer\n",
    "            - dataset list : ['imdb', 'snli', 'agnews']\n",
    "        --top : Add a top adapter to the model (same as '-t')\n",
    "        --layer : Add layer-wise adapters to the model (same as '-l')\n",
    "        --num_adapters : number of adapters (default : 12)\n",
    "            - optional: half - 6으로 주고 adapter 개수 조절\n",
    "        --full_finetuning : full fine-tuning or adapter-based tuning (boolean)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "HYPER PARAMETERS:\n",
    "    These parameters are adjustable based on your preference and requirements.\n",
    "    Feel free to adjust these settings to suit your needs.\n",
    "\"\"\"\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "VALID_RATIO = 0.1\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 5e-6\n",
    "\n",
    "\n",
    "def main():\n",
    "    # set seed\n",
    "    functions.seed_everything(RANDOM_SEED)\n",
    "    \n",
    "    # argument parsing    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--dataset', dest='dataset', action='store')\n",
    "    parser.add_argument('-t', '--top', dest='top', action='store_true')\n",
    "    parser.add_argument('-l', '--layer', dest='layer', action='store_true')\n",
    "    parser.add_argument('-n', '--num_adapters', dest='num_adapters', action='store', type=int, default=12)\n",
    "    parser.add_argument('-f', '--full_finetuning', dest='full_finetuning', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "    num_adapters = args.num_adapters\n",
    "    full_finetuning = args.full_finetuning\n",
    "    layer_wise_adapters = None\n",
    "    top_adapter = None\n",
    "\n",
    "    # load pretrained tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # load dataset, set adapters and criterion\n",
    "    if args.dataset == 'imdb':\n",
    "        train_dataset = IMDBDataset(tokenizer=tokenizer, mode='train')\n",
    "        test_dataset = IMDBDataset(tokenizer=tokenizer, mode='test')\n",
    "        num_classes = 2\n",
    "            \n",
    "    elif args.dataset == 'snli':\n",
    "        train_dataset = SNLIDataset(tokenizer=tokenizer, mode='train')\n",
    "        test_dataset = SNLIDataset(tokenizer=tokenizer, mode='test')\n",
    "        num_classes = 3\n",
    "        \n",
    "    elif args.dataset == 'agnews':\n",
    "        train_dataset = AGNewsDataset(tokenizer=tokenizer, mode='train')\n",
    "        test_dataset = AGNewsDataset(tokenizer=tokenizer, mode='test')\n",
    "        num_classes = 4\n",
    "    \n",
    "    # load model with adapters\n",
    "    if args.top:\n",
    "        top_adapter = TopAdapter(num_classes=num_classes) \n",
    "\n",
    "    if args.layer:\n",
    "        layer_wise_adapters = LayerWiseAdapter(num_adapters=num_adapters)\n",
    "        \n",
    "    #if args.full_finetuning:\n",
    "\n",
    "    model = BERT(top_adapter=top_adapter, \n",
    "                 layer_wise_adapters=layer_wise_adapters, \n",
    "                 num_classes=num_classes,\n",
    "                 num_adapters=num_adapters,\n",
    "                 full_finetuning=full_finetuning,\n",
    "                 )\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # train-valid split\n",
    "    train_size = int((1-VALID_RATIO) * len(train_dataset))  \n",
    "    val_size = len(train_dataset) - train_size  \n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # dataloaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    # train & validation\n",
    "    best_model, train_losses, train_accs, val_losses, val_accs = functions.train(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        criterion=criterion,\n",
    "        epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "    # visualize\n",
    "    functions.result(train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "    # inference\n",
    "    accuracy = functions.inference(best_model, test_dataloader)\n",
    "    print(f\"inference acc: {np.round(accuracy, 4)}\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_gpu",
   "language": "python",
   "name": "cuda_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
